## 多线程源码：线程间通信及并发

### 1、线程并发安全机制

java多线程有三大特性：原子性、可见性、有序性

**原子性：**多个操作作为一个整体，不能被分割与中断，也不能被其他线程干扰。如果被中断与干扰，则会出现数据异常、逻辑异常。

**可见性：**一个线程修改的共享变量，其他线程是否能够立刻看到。对于串行程序而言，并不存在可见性问题，前一个操作修改的变量，后一个操作一定能读取到最新值。但在多线程环境下如果没有正确的同步则不一定。

**有序性：**指从观察到的结果推测，代码最终执行的顺序与我们看到的代码的顺序不一致。

* **可见性问题代码示例**

  假设有两个线程AB，还有一个类变量int i=0。启动线程后马上再A线程中执行i=1，过两秒再B线程中获取i，取到的值可能还是0。这就是一个典型的可见性问题。其实这就是可见性问题。

  我们先来看几段代码：

  ```java
  public class volatile1 {
  
      static int i = 0;
  
      public static void main(String[] args) {
  
          new Thread(() -> {
              while (true) {
                  if (i == 1) {
                      System.out.println(Thread.currentThread().getName() + ">>>>> i = " + i);
                      break;
                  }
              }
          }, "A").start();
  
          new Thread(() -> {
              try {
                  Thread.sleep(2000);
              } catch (InterruptedException e) {
                  throw new RuntimeException(e);
              }
              i = 1;
          }, "B").start();
          
      }
  }
  ```

  以上代码是一个常见的案例，对多线程有过一些了解的同学就能知道此时A线程中的打印是无法执行的，因为此处就存在可见性问题。这时由于每个线程对于i变量都有一个属于自己的副本，A线程获取副本时i=0，B线程修改了i的值之后A线程并没有去内存中重新获取i，没有人通知A去刷新副本，所以A里的打印不会执行。

  如果将A线程的休眠时间大于B线程，即在B修改了i的值之后，A再进入判断，则可以打印。因为B修改了值之后会将这个值写回内存，此时内存中的i已经是1了，那么当A获取内存中的i值作为副本时，自然i就等于1了。

* **可见性诡异问题**

  让我们保持上面代码的其他部分，只在A线程的循环内加入一段打印。代码如下：

  ```java
  ...
          new Thread(() -> {
              while (true) {
                  System.out.println("haha");
                  if (i == 1) {
                      System.out.println(Thread.currentThread().getName() + ">>>>> i = " + i);
                      break;
                  }
              }
          }, "A").start();
  ...
  ```

  按照我们上面的理解，此处应该还是无法打印i的值才对，但事实上却可以。这是为何呢？

  网络上有人总结了几种会出现可打印的情况：

  1. 加一行`System.out.println();`
  2. 加同步块`synchronized`
  3. 加`Thread.sleep()`休眠
  4. 加一行`File file = new File(filePath);`

  以上这几种情况都会让A线程拿到真正的i值，为什么呢？有人总结是有操作io，所以可以拿到，其实不是。

  真正的原因是这几种方法都用到了synchronized关键字，无论是打印还是线程的休眠又或是File的构造方法中，都一定有synchronized关键字出现，synchronized可以保证可见性、有序性、原子性。所以是代码遇到了synchronized的时候去刷新了副本，这时i才可以拿到最新的值。以上代码中的`System.out.println("haha");`就是通知线程去刷新副本的。

* **什么时候会自动刷新副本？**

  ```java
  public class Volatile2 {
  
      static int i = 0;
      private static final int count = 600; // 100 200 300 ... 10000 100000
  
      public static void main(String[] args) {
          new Thread(() -> {
              final List<Integer> list = new ArrayList<>();
              for (int j = 1; j <= count; j++) {
                  list.add(j);
              }
  
              while (true) {
                  if (list.contains(i)) {
                      System.out.println(Thread.currentThread().getName() + ">>>> i = " + i);
                      break;
                  }
              }
          }, "A").start();
  
          new Thread(() -> {
              try {
                  Thread.sleep(1000);
              } catch (InterruptedException e) {
                  throw new RuntimeException(e);
              }
              for (int j = 1; j <= count; j++) {
                  i = j;
              }
          }, "B").start();
      }
  }
  ```

  以上代码中在A线程中首先创建出一个list并且往里面插入了1-n的数字，然后进入循环，判断list是否包含i。B线程中休眠1秒之后（为了确保A线程已经准备好list并进入循环），也开始循环给i赋值。当i达到一定值的时候，在A线程中的副本就会被刷新。在windows系统下大概是i=600的时候，A线程中打印的i值也是600。这貌似说明了当i到达600的时候就会让A线程刷新一次副本，但是我们加大数字到100000时会发现最终打印的i值不固定。可能是10W，也可能是几千。

  这个例子中就抛出了两个问题：

  1. 这个刷新副本的阈值是如何得出的，为什么到了这个值就会刷新副本？
  2. 为什么刷新副本的值是随机的，有时10W有时几千？

  这其实是由CPU多级缓存架构造成的，我们接着来详细讲解。





#### （1）volatile机制

volatile是Java提供的一种轻量级的同步机制，保证了多线程之间的可见性和有序性。在多个线程需要共享同一个变量时，此变量可用volatile关键字修饰，被修饰后的变量在修改之后所有读取它的线程都能获取到正确的值。

以上示例中如果变量被volatile修饰，即可保证可见性，即可在多个线程中正常共享变量。



volatile的底层实现非常复杂，多数是硬件级别的操作。想要真正理解volatile的原理，我们必须了解CPU多级缓存架构、JMM内存模型等知识。

理解voaltile的正确姿势：单cpu架构->cpu多级cache结构 -> **缓存一致性协议（MESI）-> store buffer和invalidate queue引入 ->造成mesi协议不一致了**-> 内存屏障-->volatile-->mesi协议再次一致

cpu芯片是非开源的

x86没有invalid queue，只有storebuffer



* **CPU多级cache结构**

  计算机在执行程序时，每条指令都是在CPU中执行的。而执行指令的过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程，跟CPU执行指令的速度比起来要慢的多（硬盘 < 内存 <缓存cache < CPU）。因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。也就是当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时，就可以直接从它的高速缓存中读取数据或向其写入数据了。当运算结束之后，再将高速缓存中的数据刷新到主存当中。

  

  目前市面上的CPU基本都是有3级缓存的，任务管理器的性能中的L1、L2、L3缓存就是，3.00GHz代表CPU每秒可以执行30亿次。

  * 为什么CPU需要三级缓存呢？

  主要是因为操作系统在处理任务时，最终是由CPU处理，但是数据是在硬盘或者内存中，内存每秒大概可执行1000W次。

  随着计算机的发展，CPU和内存之间的性能差异越来越大。

  CPU和内存的性能差距会导致系统的瓶颈被内存限制，所以随着发展CPU就开始需要缓存，开始时也只有1级缓存，发展到现在基本上CPU都是三级缓存了。

  ![1322310](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230524220635.png)

  

* **CPU缓存架构和JMM类比**

  ![image-20210219164910017](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230524221151.png)

  左边是CPU缓存架构图，右边是java线程架构图。

  我们会发现两个图的结构是一样的，这是因为在软件设计中其实很多都是参考硬件的架构设计的。

  上图中的其实是自上而下的关系：CPU --> cache --> 线程 --> 工作内存 --> 主内存

  两个CPU中的两个cache就存在可见性问题，比如在cache中将i修改，它需要让cache2知道，这其中有一个通知机制，即总线嗅探机制。在java中就是通过jmm内存模型实现。

  在CPU中每次修改了其中一个cache内的值后都会通知其他cache更新最新的值，但是不一定会刷到主内存。CPU在大部分情况下读取的都是缓存中的数据，这样才能保证性能。

  

* **什么是JMM**

   JMM 是Java内存模型（ Java Memory Model），简称JMM。它本身只是一个抽象的概念，并不真实存在，它描述的是一种规则或规范，是和多线程相关的一组规范。通过这组规范，定义了程序中对各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。需要每个JVM 的实现都要遵守这样的规范，有了JMM规范的保障，并发程序运行在不同的虚拟机上时，得到的程序结果才是安全可靠可信赖的。如果没有JMM 内存模型来规范，就可能会出现，经过不同 JVM 翻译之后，运行的结果不相同也不正确的情况。

  

* **JMM存在的必要性**

  ![7](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230524222721.png)

  关于主内存与工作内存之间的交互协议，即一个变量如何从主内存拷贝到工作内存。如何从工作内存同步到主内存中的实现细节。java内存模型定义了8种操作来完成。这8种操作每一种都是原子操作。8种操作如下：

  - lock(锁定)：作用于主内存，它把一个变量标记为一条线程独占状态；
  - read(读取)：作用于主内存，它把变量值从主内存传送到线程的工作内存中，以便随后的load动作使用；
  - load(载入)：作用于工作内存，它把read操作的值放入工作内存中的变量副本中；
  - **use(使用)：作用于工作内存，它把工作内存中的值传递给执行引擎（CPU），每当虚拟机遇到一个需要使用这个变量的指令时候，将会执行这个动作；**
  - **assign(赋值)：作用于工作内存，它把从执行引擎获取（CPU）的值赋值给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时候，执行该操作；**
  - store(存储)：作用于工作内存，它把工作内存中的一个变量传送给主内存中，以备随后的write操作使用；
  - write(写入)：作用于主内存，它把store传送值放到主内存中的变量中。
  - unlock(解锁)：作用于主内存，它将一个处于锁定状态的变量释放出来，释放后的变量才能够被其他线程锁定；

  Java内存模型还规定了执行上述8种基本操作时必须满足如下规则:

  （1）不允许read和load、store和write操作之一单独出现（即不允许一个变量从主存读取了但是工作内存不接受，或者从工作内存发起会写了但是主存不接受的情况），以上两个操作必须按顺序执行，但没有保证必须连续执行，也就是说，read与load之间、store与write之间是可插入其他指令的。

  （2）不允许一个线程丢弃它的最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。

  （3）不允许一个线程无原因地（没有发生过任何as144sign操作）把数据从线程的工作内存同步回主内存中。

  （4）一个新的变量只能从主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。

  （5）一个变量在同一个时刻只允许一条线程对其执行lock操作，但lock操作可以被同一个条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。

  （6）如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。

  （7）如果一个变量实现没有被lock操作锁定，则不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。

  （8）对一个变量执行unlock操作之前，必须先把此变量同步回主内存（执行store和write操作）。



* **CPU缓存一致性协议（mesi协议详解）**

  * **cacheline(cache block)结构与原理**

    **cacheline是对cache切分的最小单位**，它的大小和操作系统、CPU有关，一般来说一个cacheline是64个字节。每次CPU读取cache时就会读取一个cacheline。之所以设计成每次读取一个cacheline而不是每次读取一个变量是因为一般来说附近的变量会和当前变量有些关联，减少读取次数提高性能。

  * **缓存架构（cacheline与工作内存之间的对应关系）**

    1、Direct Mapped Cache：直接映射cache，比较早期的模型会使用此对应关系。下图中Address是一个32位的地址值，从2-11位用于计算index，12-31位用于计算tag。一个index就是一个cacheline，由于index的数量有限是1024个，当i和j两个变量计算出的index相同时，后一个值会将前一个值替代掉。tag就是当index相同的时候用于比较i和j是否同一个值，不同的话就将旧值替换成新值。

    ![v2-7682455a52c19fc023b3190bdff67e61_720w](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230525195758.jpg)

    

    2、Two-way Set Associative Cache：两路（多路）关联缓存，目前在硬件中使用最多的缓存结构。如果你了解过hashmap的存储原理，那么对于这种缓存架构一定不陌生，软件中的许多原理架构其实就是根据硬件参考设计的。hashmap中存储时首先是数组，hash值相同时，会发展成链表。在这种缓存架构中也是如此，下图中每一行叫做一个set，每一列叫做way（路），当计算出的set值相同并且way0已有值时，就将它放入到way1中，形成一个类似链表数组的结构。

    ![image-20210225104121219](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230525203548.png)

    ![2019062318525615](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230525203554.png)

    

  * **cpu、cache、内存交互的过程**

    ![image-20230525205620234](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230525205621.png)

    我们可以将cache的结构理解为类似上图中的结构，每一行都是一个cacheline，每一个格子中可以存放一个变量。

    CPU获取数据时先从缓存中获取，获取不到的话再由缓存从内存中获取后存储到缓存中再交给CPU。

    假设所有的缓存都被占满了，CPU要获取的值在缓存中没有，此时缓存就需要从内存中获取数据，并且根据替换策略来替换一整行的数据，也就是一个cacheline。并且这里还涉及到一个问题，就是内存中被修改过的值何时被写入到内存中。

    * cache写机制

      *缓存命中：*

      write through 直写：修改缓存后立马将数据写回主存。这种方法性能低，早期架构会采用这种方式，现在用的很少了。

      write back 回写：修改缓存后不立即写入主存，而是等到一定时候触发一次回写。虽然没有马上写回主存，但是每次修改缓存是会通过mesi协议通知其他缓存同步修改的。这种方式是目前很常见的一种机制，性能好。

      *缓存未命中：*

      write allocate ：缓存中没有值，直接修改主存，改完之后立马获取到缓存中。这个也是比较常见的。

    * cache替换策略

      1. LFU(least frequence used) 最不经常使用
      2. LRU(least recently used) 最近不常使用
      3. 随机替换

      还有一些其他的淘汰策略，不一一列举。不难发现这些淘汰策略和redis里的几乎一样，因为这些算法都是相通通用的。

      

  * **cacheline的两个局部性约束**

    从内存里读取一个cacheline的时候需要考虑处理读取本身需要的这个数据之外，还需要读取那些数据。因为一个cacheline是有一定大小的，可以读取多个变量。那么在这些变量选择上就会依据时间局部性和空间局部性来读取。

    **时间局部性**：CPU在前一个时刻访问的数据在临近的后一些时刻再被访问的概率很大。所以CPU获取数据时如果缓存中没有，需要从内存中获取，获取时也会在缓存中存储一份，方便后续获取时直接从缓存取。

    **空间局部性**：CPU访问一个数据时很可能需要此数据附近的数据。

    ![image-20230525213558055](https://gitee.com/ahongbaba/note-picture/raw/master/img/20230525213558.png)

    上图中创建了一个二维数组，通过两种赋值方式来说明cacheline的读取。

    第一个中a\[i][j]的方式，即获取的顺序是a\[0][0]、a\[0][1]、...、a\[0][n]，他们的内存地址是连续的，在CPU获取a\[0][0]时cache读取一个cacheline，那么cacheline中就会有a\[0][0]---a\[0][cacheline.size-1]个数据，这就意味着读取第二、第三个数据时无需再从内存里拿，缓存里已经有了。

    第二种方式读取的内存地址不连续，cacheline里还是读取的连续地址，相当于开始时每个cacheline里都只有第一个值有用，当程序执行了2048次要获取a\[0][1]时，最初的cacheline很大概率早就被淘汰了，因为cache中的容量是有限的。

    

  









